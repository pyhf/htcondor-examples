# interactive_gpu.sub
# Submit file to access the GPU via docker

# Must set the universe to Docker
universe = docker
# docker_image = pyhf/cuda:0.7.0-jax-cuda-11.6.0-cudnn8
docker_image = pyhf/cuda@sha256:6f8eabefa43b992a2d34e6453e97fd6c0174de852ad2e760ba69e93363efa695

# set the log, error and output files
log = interactive_gpu.log.txt
error = interactive_gpu.err.txt
output = interactive_gpu.out.txt

# interactive job so don't set any executable by default

should_transfer_files = YES
when_to_transfer_output = ON_EXIT

# We require a machine with a modern version of the CUDA driver
Requirements = (Target.CUDADriverVersion >= 11.6)

# We must request 1 CPU in addition to 1 GPU
request_cpus = 1
request_gpus = 1

# select some memory and disk space
request_memory = 2GB
request_disk = 2GB

# Opt in to using CHTC GPU Lab resources
+WantGPULab = true
# Specify short job type to run more GPUs in parallel
# Can also request "medium" or "long"
+GPUJobLength = "short"

# Tell HTCondor to run the job
queue
